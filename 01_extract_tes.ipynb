{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc1b6392",
   "metadata": {},
   "source": [
    "## Libraries etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b24b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "# #\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.sparse import csr_matrix\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from unidecode import unidecode\n",
    "from editdistance import eval as ed\n",
    "from itertools import chain, combinations\n",
    "from evlex_shared_scripts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e801cf0",
   "metadata": {},
   "source": [
    "## Part 1: preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b5cc50",
   "metadata": {},
   "source": [
    "#### A: read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0258295f-1cc6-4a84-9f46-5ab436cea1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "metadata_fn = './files/doreco_files_metadata.csv'\n",
    "corpus_fn = './files/corpus_doreco.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a98062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('traditional narrative', 626), ('personal narrative', 436), ('stimulus retelling', 158), ('conversation', 108), ('procedural', 76), ('procedural/conversation', 2)]\n"
     ]
    }
   ],
   "source": [
    "genres = {'personal narrative','traditional narrative','conversation','procedural'}\n",
    "#\n",
    "raw_corpus = pickle.load(open(corpus_fn, 'rb'))    \n",
    "corpus = select_genres(raw_corpus, metadata_fn, genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af128bd-b57c-47b5-bdd7-185dd66de5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wct = Counter()\n",
    "for doc in corpus:\n",
    "    for file in corpus[doc]:\n",
    "        for line, elt in corpus[doc][file].items():\n",
    "            wct[doc[:-4]] += len(elt['spc'])\n",
    "with open('./files/word_counts.csv', 'w') as fh:\n",
    "    fh.write('doculect,count\\n' + '\\n'.join('%s,%d' % i for i in wct.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbdafc2e-8e16-422a-86f8-0a98eb2da97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kama1351', 77445),\n",
       " ('bora1263', 53905),\n",
       " ('kaka1265', 53428),\n",
       " ('komn1238', 47393),\n",
       " ('goem1240', 46673),\n",
       " ('sout3282', 44201),\n",
       " ('nngg1234', 40876),\n",
       " ('beja1238', 38121),\n",
       " ('arap1274', 35839),\n",
       " ('dolg1241', 34863),\n",
       " ('trin1278', 31221),\n",
       " ('goro1270', 31186),\n",
       " ('urum1249', 29006),\n",
       " ('sout2856', 26969),\n",
       " ('yong1270', 26326),\n",
       " ('vera1241', 23973),\n",
       " ('bain1259', 22733),\n",
       " ('cash1254', 21875),\n",
       " ('pnar1238', 20796),\n",
       " ('ruul1235', 20133),\n",
       " ('anal1239', 20041),\n",
       " ('even1259', 19877),\n",
       " ('sanz1248', 19693),\n",
       " ('svan1243', 19313),\n",
       " ('orko1234', 19309),\n",
       " ('texi1237', 19218),\n",
       " ('nort2641', 18824),\n",
       " ('apah1238', 18726),\n",
       " ('nisv1234', 18217),\n",
       " ('kark1256', 17835),\n",
       " ('movi1243', 17308),\n",
       " ('teop1238', 16777),\n",
       " ('nort2875', 15818),\n",
       " ('tsim1256', 14529),\n",
       " ('port1286', 14281),\n",
       " ('jeju1234', 14169),\n",
       " ('lowe1385', 13814),\n",
       " ('jeha1242', 13060),\n",
       " ('sadu1234', 12286),\n",
       " ('resi1247', 11552),\n",
       " ('savo1255', 10897),\n",
       " ('hoch1243', 10729),\n",
       " ('sumi1235', 10345),\n",
       " ('yuca1254', 9936),\n",
       " ('taba1259', 9346),\n",
       " ('cabe1245', 8472),\n",
       " ('yura1255', 8128),\n",
       " ('ngal1292', 5183)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wct.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e21aa-5e07-4164-bc5b-9e509c371890",
   "metadata": {},
   "source": [
    "## B: extract TEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5803ab59-dbe8-43c8-82bd-d178a13dd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fragment extraction\n",
    "\n",
    "def get_fragments(w, min_len=2, max_len=7, yield_self=True):\n",
    "    \"\"\"\n",
    "    takes a string *w* and extracts all substrings of minimal length *min_len* and maximal length *max_len*\n",
    "    \"\"\"\n",
    "    if yield_self and len(w) > max_len: yield w\n",
    "    for i in range(len(w)):\n",
    "        for j in range(i+min_len, min(i+max_len+1,len(w)+1)):\n",
    "            if (not (i == 0 or j == len(w))) or (j-i) >= min_len+1:\n",
    "                yield w[i:j]\n",
    "\n",
    "def get_all_fragments(F, split_words = True, frequency_threshold = 1):\n",
    "    \"\"\"\n",
    "    given a document stored in a dictionary *tbd*, mapping an identifier key \n",
    "    onto a string containing the text, and a set of *all_verses*\n",
    "    (the shared identifier keys between tbd and the source document(,\n",
    "    this function returns a sparse matrix *fragments* of identifier key (rows) \n",
    "    to substrings of the text (column), with the matrix being True if the fragment\n",
    "    occurs for that identifier key and False otherwise.\n",
    "    as well as dictionaries for the identification\n",
    "    of the rows and columns.\n",
    "    (memory/computation efficient format, but a bit densely written)\n",
    "    \"\"\"\n",
    "    wordcount = Counter((w for l in F for w in l))\n",
    "    if split_words: \n",
    "        word_fragments = {w : set(get_fragments('^%s$' % unidecode(w).lower())) for w in wordcount.keys() }\n",
    "    else: \n",
    "        word_fragments = {w : {unidecode(w).lower()} for w in wordcount.keys() }\n",
    "    \n",
    "    fragment_count = Counter((f for w,F in word_fragments.items() for f in F if f != '' for i in range(wordcount[w])))\n",
    "    fragment_ixx = {f:i for i,(f,c) in enumerate(fragment_count.most_common()) if c >= frequency_threshold }\n",
    "    #\n",
    "    R,C = [], []\n",
    "    for line_ix, line_f in enumerate(F):\n",
    "        if len(line_f) == 0: continue\n",
    "        line_frags = list(map(lambda f : fragment_ixx[f],\n",
    "                              filter(lambda f : f in fragment_ixx,\n",
    "                                     set.union(*map(lambda w : word_fragments[w], line_f)))))\n",
    "        R.extend([line_ix]*len(line_frags))\n",
    "        C.extend(line_frags)\n",
    "    fragments = csr_matrix((np.ones(len(R)), (R,C)), dtype=bool, shape = (len(F),max(C)+1))\n",
    "    return fragments, fragment_ixx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88cde75f-2e7f-44ae-a456-5870c762f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alignments(doc, corpus, min_freq = 1):\n",
    "    #\n",
    "    E, F, I = zip(*((get_lexical(get_mappable(e['spc'])), e['tx'].split(), (f,l))\n",
    "                    for f in corpus[doc] for l,e in corpus[doc][f].items()))\n",
    "    #for e,f,i in zip(E,F,I[:5]): print(i,'\\n\\t',e,'\\n\\t',f)\n",
    "    e_fragments, e_dic = get_all_fragments(E, split_words=False, frequency_threshold=1)\n",
    "    e_counts = Counter([unidecode(e).lower() for l in E for e in l if e != ''])\n",
    "    e_seed = {e : e_dic[e] for e in e_counts if e_counts[e] >= min_freq and e != ''}\n",
    "    f_fragments, f_dic = get_all_fragments(F, split_words=True, frequency_threshold=1)\n",
    "    f_list = np.array(sorted(f_dic, key = lambda k : f_dic[k]))\n",
    "    #\n",
    "    # get TEs\n",
    "    tes, te_words = {}, {}\n",
    "    for e,ei in sorted(e_seed.items(), key = lambda x : -e_counts[x[0]]):\n",
    "        pos = e_fragments[:,ei].nonzero()[0]\n",
    "        tes[e] = extract_tes(pos, f_fragments, f_dic, coverage=.95, min_trans=0.01, min_backtrans=0.10)\n",
    "        #tes[e] = merge_similar_tes(tes[e])\n",
    "        te_words[e] = { te : get_te_words(f_fragments, f_list, te, te_pos) for te,te_pos in tes[e].items() }\n",
    "        # print(e, ei, e_counts[e], len(pos),'\\n\\t', tes[e].keys(), te_words[e])\n",
    "    print(doc, datetime.now(), len(tes))\n",
    "    pickle.dump((tes, te_words), open('./pickles/tes_%s.p'%doc,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2affcc-d04f-4670-8aa2-13accefb372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tes(pos_ixx, fragments, fixx, coverage = 0.95, min_trans = 0.05, min_backtrans = 0.25, verbose=False):\n",
    "    \"\"\"\n",
    "    implements one forward pass of the Liu et al. (2023) approach.\n",
    "    Takes a list *pos* of positive instances (row identifiers for fragments)\n",
    "    As well as the sparse matrix *fragments* and the two dictionaries\n",
    "    (vixx -- verse_ixx and fixx -- fragment_ixx).\n",
    "    The parameters determine the fragments considered in the extraction: the iteration keeps going until\n",
    "    either no good fragments can be found (significance under .001) or the *coverage* has been reached.\n",
    "    *min_trans* is a float [0,1] that filters out all fragments that occur in hit verses in less than min_trans\n",
    "    proportion of all hit verses.\n",
    "    \"\"\"\n",
    "    neg_ixx = list(set(range(fragments.shape[0]))-set(pos_ixx))\n",
    "    flist = [None] * len(fixx)\n",
    "    for k,v in fixx.items():\n",
    "        flist[v] = k\n",
    "    #sorted(fixx, key = lambda k : fixx[k])\n",
    "    #\n",
    "    pos_tot_orig = pos_tot = len(pos_ixx)\n",
    "    neg_tot = len(neg_ixx)\n",
    "    #\n",
    "    pos_ct = fragments[pos_ixx].sum(0).A[0]\n",
    "    neg_ct = fragments[neg_ixx].sum(0).A[0]\n",
    "    #\n",
    "    good_fragments = np.where((pos_ct >= 1) & ((pos_ct/pos_tot_orig) >= min_trans) &\n",
    "                             (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
    "    string_props = [(f[0] == '^', f[-1] == '$', len(f)) for f in flist]\n",
    "                    #len(re.sub('[.*]', '', f)))\n",
    "    #\n",
    "    hits = defaultdict(lambda : [])\n",
    "    ct = 0\n",
    "    fe_scores = {}\n",
    "    while len(pos_ixx) >= (1-coverage) * pos_tot_orig:\n",
    "        ct += 1\n",
    "        #\n",
    "        # GET BEST\n",
    "        assoc_scores = Counter()\n",
    "        for f in good_fragments:\n",
    "            table = ((pos_ct[f],pos_tot-pos_ct[f]),(neg_ct[f],neg_tot-neg_ct[f]))\n",
    "            try: fe_score = fe_scores[table]\n",
    "            except KeyError: fe_score = fe_scores[table] = -np.log(fisher_exact(table, alternative='greater')[1])\n",
    "            assoc_scores[f] = (fe_score, string_props[f])\n",
    "        best, best_score = next((x for x in assoc_scores.most_common(1)),(None,None))\n",
    "        if best == None or best_score[0] < -np.log(5e-2):\n",
    "            break\n",
    "        # print([flist[k] for k,v in assoc_scores.most_common(10)])\n",
    "        #\n",
    "        # UPDATE\n",
    "        new_pos_ixx = []\n",
    "        for pos_v in pos_ixx:\n",
    "            if fragments[pos_v,best] > 0: hits[flist[best]].append(pos_v)\n",
    "            else: new_pos_ixx.append(pos_v)\n",
    "        #\n",
    "        pos_ixx = new_pos_ixx\n",
    "        pos_tot = len(pos_ixx)\n",
    "        pos_ct = fragments[pos_ixx].sum(0).A[0]\n",
    "        neg_ct = fragments[neg_ixx].sum(0).A[0]\n",
    "        #\n",
    "        good_fragments = np.where((pos_ct >= 1) & (pos_ct/pos_tot_orig >= min_trans) &\n",
    "                                    (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
    "        if verbose: print(ct, flist[best])\n",
    "    return hits\n",
    "\n",
    "def merge_similar_tes(tes):\n",
    "    merge = nx.Graph()\n",
    "    for t1, t2 in combinations(tes,2):\n",
    "        if t1 in t2 or t2 in t1 or ed(t1,t2) <= 1:\n",
    "            merge.add_edge(t1,t2)\n",
    "    for c in nx.connected_components(merge):\n",
    "        if len(c) >= 2:\n",
    "            best_c = max(c, key= lambda k : len(tes[k]))\n",
    "            tes[best_c] = list(chain(*[tes[ci] for ci in c]))\n",
    "            for ci in filter(lambda ci : ci != best_c, c): del tes[ci]\n",
    "            #print('>>> MERGED', c)\n",
    "    return tes\n",
    "\n",
    "def longest_nonoverlapping(m, rev, frags):\n",
    "    frags = [rev[f] for f in frags]\n",
    "    Ma = [mi for mi in frags if m in mi and \n",
    "          next((False for mj in frags if mj != mi and mi in mj),True)]\n",
    "    return Ma\n",
    "\n",
    "def get_te_words(fragments_F, rev, te, te_pos):\n",
    "    ctr = Counter()\n",
    "    for pos in te_pos:\n",
    "        frags = fragments_F[pos].nonzero()[1]\n",
    "        Ma = longest_nonoverlapping(te, rev, frags)\n",
    "        for m in Ma:\n",
    "            ctr[m] += 1\n",
    "    return ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200a83f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1515612/1182429609.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anal1239.csv 2024-10-28 15:49:34.897733 1125\n",
      "apah1238.csv 2024-10-28 15:49:44.041965 779\n",
      "arap1274.csv 2024-10-28 15:50:08.026997 953\n",
      "bain1259.csv 2024-10-28 15:50:27.626472 1167\n",
      "beja1238.csv 2024-10-28 15:51:07.188586 1247\n",
      "bora1263.csv 2024-10-28 15:52:20.130860 1815\n",
      "cabe1245.csv 2024-10-28 15:52:25.844092 457\n",
      "cash1254.csv 2024-10-28 15:52:38.408719 883\n",
      "dolg1241.csv 2024-10-28 15:53:11.050525 1343\n",
      "even1259.csv 2024-10-28 15:53:32.183058 948\n",
      "goem1240.csv 2024-10-28 15:53:40.005157 893\n",
      "goro1270.csv 2024-10-28 15:53:56.814768 1065\n",
      "hoch1243.csv 2024-10-28 15:54:04.291441 558\n",
      "jeha1242.csv 2024-10-28 15:54:08.221180 485\n",
      "jeju1234.csv 2024-10-28 15:54:17.697955 777\n",
      "kaka1265.csv 2024-10-28 15:54:43.425064 1633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1515612/1182429609.py:40: RuntimeWarning: divide by zero encountered in log\n",
      "  except KeyError: fe_score = fe_scores[table] = -np.log(fisher_exact(table, alternative='greater')[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kama1351.csv 2024-10-28 15:55:22.891528 1336\n",
      "kark1256.csv 2024-10-28 15:55:35.528618 812\n",
      "komn1238.csv 2024-10-28 15:56:12.826234 1366\n",
      "lowe1385.csv 2024-10-28 15:56:21.064941 863\n",
      "movi1243.csv 2024-10-28 15:56:32.208920 825\n",
      "ngal1292.csv 2024-10-28 15:56:35.100712 358\n",
      "nisv1234.csv 2024-10-28 15:56:43.218431 724\n",
      "nngg1234.csv 2024-10-28 15:56:53.716290 920\n",
      "nort2641.csv 2024-10-28 15:57:03.368138 980\n",
      "nort2875.csv 2024-10-28 15:57:13.506871 881\n",
      "orko1234.csv 2024-10-28 15:57:19.389148 646\n",
      "pnar1238.csv 2024-10-28 15:57:31.424915 1108\n",
      "port1286.csv 2024-10-28 15:57:35.619906 561\n",
      "resi1247.csv 2024-10-28 15:57:42.819715 509\n",
      "ruul1235.csv 2024-10-28 15:58:03.036831 1162\n",
      "sadu1234.csv 2024-10-28 15:58:11.377139 942\n",
      "sanz1248.csv 2024-10-28 15:58:40.838646 1105\n",
      "savo1255.csv 2024-10-28 15:58:45.284982 542\n",
      "sout2856.csv 2024-10-28 15:58:56.545013 848\n",
      "sout3282.csv 2024-10-28 15:59:23.088508 1968\n",
      "sumi1235.csv 2024-10-28 15:59:27.458430 552\n",
      "svan1243.csv 2024-10-28 15:59:47.754157 1150\n",
      "taba1259.csv 2024-10-28 15:59:55.171925 565\n",
      "teop1238.csv 2024-10-28 15:59:59.432056 599\n",
      "texi1237.csv 2024-10-28 16:00:06.609480 573\n",
      "trin1278.csv 2024-10-28 16:00:34.787429 1202\n",
      "tsim1256.csv 2024-10-28 16:00:45.131407 641\n",
      "urum1249.csv 2024-10-28 16:01:08.337683 942\n",
      "vera1241.csv 2024-10-28 16:01:15.103738 717\n",
      "yong1270.csv 2024-10-28 16:01:26.610310 997\n",
      "yuca1254.csv 2024-10-28 16:01:33.909739 748\n",
      "yura1255.csv 2024-10-28 16:01:36.858816 319\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    create_alignments(doc, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4c534-3834-4e50-a58a-cfaac63d3c44",
   "metadata": {},
   "source": [
    "## post-hoc merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3a56cec-9e65-4f36-b1fe-903482dff5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations, chain\n",
    "from editdistance import eval as ed\n",
    "from scipy.stats import entropy\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def JSD(P, Q):\n",
    "    _P = P / norm(P, ord=1)\n",
    "    _Q = Q / norm(Q, ord=1)\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
    "\n",
    "def merge_similar_tes_within(tes, te_words):\n",
    "    merge = nx.Graph()\n",
    "    merge.add_nodes_from(tes)\n",
    "    tes_new, te_words_new = {}, {}\n",
    "    for t1, t2 in combinations(tes,2):\n",
    "        if t1 in t2 or t2 in t1 or ed(t1,t2) <= 1:\n",
    "            merge.add_edge(t1,t2)\n",
    "    for c in nx.connected_components(merge):\n",
    "        best_c = max(c, key= lambda k : len(tes[k]))\n",
    "        tes_new[best_c] = list(chain(*[tes[ci] for ci in c]))\n",
    "        te_words_new[best_c] = sum([te_words[ci] for ci in c], Counter())\n",
    "        #print('>>> MERGED', c)\n",
    "    return tes_new, te_words_new\n",
    "\n",
    "def merge_similar_tes_across(tes, te_words):\n",
    "    merge_markers = nx.Graph()\n",
    "    te_marker_pairs = [(te, marker) for te in tes for marker in tes[te] if len(tes[te][marker]) >= 5]\n",
    "    for (tei,mrki),(tej,mrkj) in combinations(te_marker_pairs,2):\n",
    "        if tei==tej: continue\n",
    "        if set(te_words[tei][mrki]) & set(te_words[tej][mrkj]) != set():\n",
    "            mrkic, mrkjc = mrki.strip('^$'), mrkj.strip('^$')\n",
    "            minlen = min(len(mrkic),len(mrkjc))\n",
    "            wi = te_words[tei][mrki]\n",
    "            wj = te_words[tej][mrkj]\n",
    "            jsd = (JSD(*zip(*[[wi[x],wj[x]] for x in sorted(set(wi)|set(wj))])))\n",
    "            cos = (wv.wv.similarity(tei,tej))\n",
    "            jac = len(set(wi)&set(wj))/len(set(wi)|set(wj))\n",
    "            form = mrkic[:minlen] == mrkjc[:minlen] or mrkic[-minlen:] == mrkjc[-minlen:]\n",
    "            if cos >= 2/3 and jsd <= 2/3 and form:\n",
    "                #print((tei,mrki),(tej,mrkj),'JSD=%.2f COS=%.2f JAC=%.2f FORM=%d' % (jsd, cos, jac,form))\n",
    "                merge_markers.add_edge((tei,mrki), (tej,mrkj))\n",
    "    for c in nx.connected_components(merge_markers):\n",
    "        new_mrk = min([mrk for te,mrk in c],key = len)\n",
    "        for te,mrk in c:\n",
    "            tes[te][new_mrk] = tes[te][mrk]\n",
    "            te_words[te][new_mrk] = te_words[te][mrk]\n",
    "            if mrk != new_mrk: del tes[te][mrk], te_words[te][mrk]\n",
    "    return tes, te_words\n",
    "\n",
    "def merge_similar_tes(doc):\n",
    "    tes, te_words = pickle.load(open('./pickles/tes_%s.p' % doc,'rb'))\n",
    "    for k,v in tes.items():\n",
    "        tes[k], te_words[k] = merge_similar_tes_within(v, te_words[k])\n",
    "    #\n",
    "    tes, te_words = merge_similar_tes_across(tes, te_words)\n",
    "    return tes, te_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
